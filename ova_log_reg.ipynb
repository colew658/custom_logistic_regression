{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom One-Versus-All Logistic Regression\n",
    "\n",
    "By: Haiyan Cai, Joe Sellett, and Cole Wagner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results. For example, would the model be deployed or used mostly for offline analysis? As in previous labs, also detail how good the classifier needs to perform in order to be useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] (mostly the same processes as from previous labs) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis (give reasoning). Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). Provide a breakdown of the variables after preprocessing (such as the mean, std, etc. for all variables, including numeric and categorical). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Divide your data into training and testing splits using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "from scipy.special import expit\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Class (Steepest Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** Documentation and type hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    \"\"\"Binary Logistic Regression using gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Constant applied to the regularization term.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eta: float,\n",
    "        iterations: int = 20,\n",
    "        C: float = 0.001,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the BinaryLogisticRegression object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : float\n",
    "            Learning rate.\n",
    "        iterations : int, optional (default=20)\n",
    "            Number of iterations for the solver.\n",
    "        C : float, optional (default=0.001)\n",
    "            Constant applied to the regularization term.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return a message for the BinaryLogisticRegression object.\"\"\"\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        return \"Untrained Binary Logistic Regression Object\"\n",
    "\n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X: np.array) -> np.array:\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta: np.array) -> np.array:\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self, X: np.array, y: np.array) -> np.array:\n",
    "        ydiff = (\n",
    "            y - self.predict_proba(X, add_bias=False).ravel()\n",
    "        )  # get y difference\n",
    "        gradient = np.mean(\n",
    "            X * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    # public:\n",
    "    def predict_proba(\n",
    "        self, X: np.array, add_bias: bool = True\n",
    "    ) -> np.array:\n",
    "        \"\"\"Predict the probability of the positive class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "        add_bias : bool, optional (default=True)\n",
    "            Whether to add a bias term to the input data.\n",
    "\n",
    "        \"\"\"\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the discrete labels based on a cutoff of p > 0.5.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        \"\"\"Fit the model to data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "        y : np.array\n",
    "            Target labels.\n",
    "\n",
    "        \"\"\"\n",
    "        Xb = self._add_bias(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros(\n",
    "            (num_features, 1)\n",
    "        )  # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta  # multiply by learning rate\n",
    "            # add bacause maximizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** documentation and static typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    \"\"\"Logistic Regression using stochastic gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Constant applied to the regularization term.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # stochastic gradient calculation\n",
    "    def _get_gradient(self, X: np.array, y: np.array) -> np.array:\n",
    "        # grab a subset of samples in a mini-batch\n",
    "        mini_batch_size = 50\n",
    "        idxs = np.random.choice(len(y), mini_batch_size)\n",
    "\n",
    "        ydiff = (\n",
    "            y[idxs] - self.predict_proba(X[idxs], add_bias=False).ravel()\n",
    "        )  # get y difference (now scalar)\n",
    "        gradient = np.mean(\n",
    "            X[idxs] * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** documentation and static typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \"\"\"Logistic Regression using Newton's method for optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Constant applied to the regularization term.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self, X: np.array, y: np.array) -> np.array:\n",
    "        g = self.predict_proba(\n",
    "            X, add_bias=False\n",
    "        ).ravel()  # get sigmoid value for all classes\n",
    "        hessian = (\n",
    "            X.T @ np.diag(g * (1 - g)) @ X - 2 * self.C\n",
    "        )  # calculate the hessian\n",
    "\n",
    "        ydiff = y - g  # get y difference\n",
    "        gradient = np.sum(\n",
    "            X * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        # Note the pinv() to make the hessian function inverse\n",
    "        return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** Added documentation, refactored `predict_proba()` to use list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    \"\"\"MultiClass Logistic Regression using One-Versus-All approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.0001)\n",
    "        Constant applied to the regularization term.\n",
    "    solver : class, optional (default=HessianBinaryLogisticRegression)\n",
    "        Solver class to use for binary logistic regression.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eta: float,\n",
    "        iterations: int = 20,\n",
    "        C: float = 0.0001,\n",
    "        solver: any = HessianBinaryLogisticRegression,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the MultiClassLogisticRegression object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : float\n",
    "            Learning rate.\n",
    "        iterations : int, optional (default=20)\n",
    "            Number of iterations for the solver.\n",
    "        C : float, optional (default=0.0001)\n",
    "            Constant applied to the regularization term.\n",
    "        solver : class, optional (default=HessianBinaryLogisticRegression)\n",
    "            Solver class to use for binary logistic regression.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return a message for the MultiClassLogisticRegression object.\"\"\"\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"MultiClass Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        return \"Untrained MultiClass Logistic Regression Object\"\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        \"\"\"Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "        y : np.array\n",
    "            Target labels.\n",
    "\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y))  # get each unique class value\n",
    "        self.classifiers_ = []\n",
    "        for i, yval in enumerate(self.unique_):  # for each unique value\n",
    "            y_binary = np.array(y == yval).astype(\n",
    "                int\n",
    "            )  # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "\n",
    "            hblr = self.solver(\n",
    "                eta=self.eta, iterations=self.iters, C=self.C\n",
    "            )\n",
    "            hblr.fit(X, y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "    def predict_proba(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the probability of each class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "\n",
    "        \"\"\"\n",
    "        probs = [\n",
    "            hblr.predict_proba(X).reshape((len(X), 1))\n",
    "            for hblr in self.classifiers_\n",
    "        ]\n",
    "        return np.hstack(probs)  # make into single matrix\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the discrete labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.unique_[\n",
    "            np.argmax(self.predict_proba(X), axis=1)\n",
    "        ]  # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ -9.78265133   2.08829579   2.65681509  -2.63973866  -2.35725176]\n",
      " [  8.85621934  -0.38997276  -3.14389254   1.37751022  -2.69916585]\n",
      " [-12.91894208  -1.01274131   0.17373293   2.11562596   5.07584625]]\n",
      "Accuracy of:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "ds = load_iris()\n",
    "X = ds.data\n",
    "\n",
    "# X = StandardScaler().fit(X).transform(X)\n",
    "y_not_binary = (\n",
    "    ds.target\n",
    ")  # note problem is NOT binary anymore, there are three classes!\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_not_binary, train_size=0.8, test_size=0.2\n",
    ")\n",
    "\n",
    "\n",
    "lr = MultiClassLogisticRegression(\n",
    "    eta=1.0,\n",
    "    iterations=4,\n",
    "    C=0.01,\n",
    "    solver=HessianBinaryLogisticRegression,\n",
    ")\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "print(\"Accuracy of: \", accuracy_score(y_test, yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
