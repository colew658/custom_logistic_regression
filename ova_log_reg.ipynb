{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom One-Versus-All Logistic Regression\n",
    "\n",
    "By: Haiyan Cai, Joe Sellett, and Cole Wagner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import pinv\n",
    "from scipy.special import expit\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv(\"customer_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results. For example, would the model be deployed or used mostly for offline analysis? As in previous labs, also detail how good the classifier needs to perform in order to be useful.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was collected by an automobile company for the purpose of customer segmentation. Customer segmentation is used by businesses to group customers into categories based on their purchasing behaviors. These behaviors may be influenced by factors such as income levels, previous spending habits, demographics, or any other data the company can obtain. This dataset was gathered because the automobile company aims to expand its current product offerings into new markets and seeks to understand potential customers in these markets. The company has successfully categorized its existing customers into four segments (A, B, C, D). The goal of our project is to accurately identify customer segments based on various variables, including gender, age, work experience, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer segmentation is a crucial practice for marketers across all industries. A successful categorization of customer segments will be highly valuable to automobile manufacturers worldwide, enabling them to determine how to allocate their marketing resources and tailor their strategies effectively. To be considered useful, our model must outperform existing models. This dataset was previously used in a competition where the winners achieved 95% accuracy in identifying customer segments. However, we believe a model with an AUC above 0.8 would generally be considered valuable in this business context, while a model achieving an AUC above 0.9 would certainly be considered highly effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[.5 points] (mostly the same processes as from previous labs) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis (give reasoning). Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). Provide a breakdown of the variables after preprocessing (such as the mean, std, etc. for all variables, including numeric and categorical).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off, we will remove the ID and Var_1 variables from the data. The ID variable is unique to each row, so it does not provide any modeling value. Var_1 is anonymised, so we cannot know what it represents. There are also 796 duplicate rows which will be removed. There are a relatively small number of missing values in the ever_married, graduated, work_experience and family_size columns which we will impute with knn impute. Profession is the only column we will impute with mean, as there are many different professions. These ended up being imputed as “artist”, as there are over 2000 customers listed as artists in this dataset. Finally, we one-hot encoded our categorical variables (“gender”, “ever_married”, “graduated”, “profession”, “spending_score”) so that it would run smoothly in our model. Now, we are ready to move forward with training our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ever_Married</th>\n",
       "      <th>Age</th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Spending_Score</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Var_1</th>\n",
       "      <th>Segmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>462809</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>22</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Cat_4</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>462643</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Average</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Cat_4</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>466315</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>461735</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>462669</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Gender Ever_Married  Age Graduated     Profession  Work_Experience  \\\n",
       "0  462809    Male           No   22        No     Healthcare              1.0   \n",
       "1  462643  Female          Yes   38       Yes       Engineer              NaN   \n",
       "2  466315  Female          Yes   67       Yes       Engineer              1.0   \n",
       "3  461735    Male          Yes   67       Yes         Lawyer              0.0   \n",
       "4  462669  Female          Yes   40       Yes  Entertainment              NaN   \n",
       "\n",
       "  Spending_Score  Family_Size  Var_1 Segmentation  \n",
       "0            Low          4.0  Cat_4            D  \n",
       "1        Average          3.0  Cat_4            A  \n",
       "2            Low          1.0  Cat_6            B  \n",
       "3           High          2.0  Cat_6            B  \n",
       "4           High          6.0  Cat_6            A  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the ID and Var_1 variables from the data. The ID variable is unique to each row, so it does not provide any modeling value. Var_1 is anonymised, so we cannot know what it represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_filtered = customers.drop(columns=[\"ID\", \"Var_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "719"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(customers_filtered.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code, we can see that we have 719 duplicate rows, which we will remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_filtered.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender               0\n",
       "Ever_Married       140\n",
       "Age                  0\n",
       "Graduated           77\n",
       "Profession         123\n",
       "Work_Experience    786\n",
       "Spending_Score       0\n",
       "Family_Size        326\n",
       "Segmentation         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many rounds of testing various imputation techniques (KNN, median, mode, etc.), we found that our test accuracy was consistently best when we simply removed all rows with missing values. This is likely due to the fact that the missing values are not random, and removing them allows us to avoid introducing bias into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_filtered = customers_filtered.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prepare the data for modeling, we need to normalize all numeric columns and one-hot encode all categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"Age\", \"Family_Size\"]\n",
    "cat_cols = customers_filtered.columns.difference(\n",
    "    [*num_cols, \"Segmentation\"],\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical columns\n",
    "scaler = StandardScaler()\n",
    "customers_filtered[num_cols] = scaler.fit_transform(\n",
    "    customers_filtered[num_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_cat_cols = one_hot_encoder.fit_transform(\n",
    "    customers_filtered[cat_cols]\n",
    ")\n",
    "\n",
    "# Create a dataframe with the encoded columns\n",
    "encoded_cat_df = pd.DataFrame(\n",
    "    encoded_cat_cols,\n",
    "    columns=one_hot_encoder.get_feature_names_out(cat_cols),\n",
    "    index=customers_filtered.index,  # Ensure the index matches the original dataframe\n",
    ")\n",
    "\n",
    "# Concatenate the encoded columns with the original dataframe\n",
    "customers_filtered = pd.concat(\n",
    "    [customers_filtered.drop(columns=cat_cols), encoded_cat_df], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[.5 points] Divide your data into training and testing splits using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    customers_filtered.drop(columns=[\"Segmentation\"]),\n",
    "    customers_filtered[\"Segmentation\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=customers_filtered[\"Segmentation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would argue that this dataset is too small to do an 80/20 train/test split. Statistician Frank Harrell claims that around 20,000 rows are needed to perform a train/test split, and we only have about 6,000 rows. In this case, we favor using cross-validation because it allows us to use all of our data for model training while still quantifying model performance on many simulated test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Class (Steepest Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** Documentation and type hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    \"\"\"Binary Logistic Regression using gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Constant applied to the regularization term.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eta: float,\n",
    "        iterations: int = 20,\n",
    "        C: float = 0.001,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the BinaryLogisticRegression object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : float\n",
    "            Learning rate.\n",
    "        iterations : int, optional (default=20)\n",
    "            Number of iterations for the solver.\n",
    "        C : float, optional (default=0.001)\n",
    "            Constant applied to the regularization term.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return a message for the BinaryLogisticRegression object.\"\"\"\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        return \"Untrained Binary Logistic Regression Object\"\n",
    "\n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X: np.array) -> np.array:\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta: np.array) -> np.array:\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "    def _get_gradient(self, X, y):\n",
    "        ydiff = (\n",
    "            y - self.predict_proba(X, add_bias=False).ravel()\n",
    "        )  # get y difference\n",
    "        gradient = np.mean(\n",
    "            X * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization == \"L1\":\n",
    "            gradient[1:] += -self.C * np.sign(self.w_[1:])\n",
    "        elif self.regularization == \"L2\":\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization == \"Both\":\n",
    "            gradient[1:] += (\n",
    "                -self.C * np.sign(self.w_[1:]) - 2 * self.w_[1:] * self.C\n",
    "            )\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    # public:\n",
    "    def predict_proba(\n",
    "        self, X: np.array, add_bias: bool = True\n",
    "    ) -> np.array:\n",
    "        \"\"\"Predict the probability of the positive class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "        add_bias : bool, optional (default=True)\n",
    "            Whether to add a bias term to the input data.\n",
    "\n",
    "        \"\"\"\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the discrete labels based on a cutoff of p > 0.5.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        \"\"\"Fit the model to data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "        y : np.array\n",
    "            Target labels.\n",
    "\n",
    "        \"\"\"\n",
    "        Xb = self._add_bias(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros(\n",
    "            (num_features, 1)\n",
    "        )  # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta  # multiply by learning rate\n",
    "            # add bacause maximizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** documentation and static typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    \"\"\"Logistic Regression using stochastic gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Constant applied to the regularization term.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # stochastic gradient calculation\n",
    "    def _get_gradient(self, X: np.array, y: np.array) -> np.array:\n",
    "        # grab a subset of samples in a mini-batch\n",
    "        mini_batch_size = 50\n",
    "        idxs = np.random.choice(len(y), mini_batch_size)\n",
    "\n",
    "        ydiff = (\n",
    "            y[idxs] - self.predict_proba(X[idxs], add_bias=False).ravel()\n",
    "        )  # get y difference (now scalar)\n",
    "        gradient = np.mean(\n",
    "            X[idxs] * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization == \"L1\":\n",
    "            gradient[1:] += -self.C * np.sign(self.w_[1:])\n",
    "        elif self.regularization == \"L2\":\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization == \"Both\":\n",
    "            gradient[1:] += (\n",
    "                -self.C * np.sign(self.w_[1:]) - 2 * self.w_[1:] * self.C\n",
    "            )\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method Using Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** documentation and static typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \"\"\"Logistic Regression using Newton's method for optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Constant applied to the regularization term.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self, X: np.array, y: np.array) -> np.array:\n",
    "        g = self.predict_proba(\n",
    "            X, add_bias=False\n",
    "        ).ravel()  # get sigmoid value for all classes\n",
    "        hessian = (\n",
    "            X.T @ np.diag(g * (1 - g)) @ X - 2 * self.C\n",
    "        )  # calculate the hessian\n",
    "\n",
    "        ydiff = y - g  # get y difference\n",
    "        gradient = np.sum(\n",
    "            X * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization == \"L1\":\n",
    "            gradient[1:] += -self.C * np.sign(self.w_[1:])\n",
    "        elif self.regularization == \"L2\":\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization == \"Both\":\n",
    "            gradient[1:] += (\n",
    "                -self.C * np.sign(self.w_[1:]) - 2 * self.w_[1:] * self.C\n",
    "            )\n",
    "\n",
    "        # Note the pinv() to make the hessian function inverse\n",
    "        return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method Using Mean Square Error as Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** documentation, static typing, and implementation of MSE as objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianBinaryLogisticRegressionMSE(BinaryLogisticRegression):\n",
    "    \"\"\"Logistic Regression using Newton's method optimized with Mean Squared Error (MSE).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Regularization constant.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_gradient(self, X: np.array, y: np.array) -> np.array:\n",
    "        g = self.predict_proba(\n",
    "            X, add_bias=False\n",
    "        ).ravel()  # get sigmoid value for all samples\n",
    "\n",
    "        # Compute Mean Squared Error gradient\n",
    "        ydiff = y - g  # error term\n",
    "        gradient = (\n",
    "            -2 * (X.T @ (ydiff * g * (1 - g))) / len(y)\n",
    "        )  # Apply MSE gradient formula\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C  # Regularization term\n",
    "\n",
    "        # Compute Hessian based on MSE\n",
    "        D = np.diag((2 * g * (1 - g)) / len(y))  # Diagonal matrix\n",
    "        hessian = X.T @ D @ X - 2 * self.C * np.eye(\n",
    "            X.shape[1]\n",
    "        )  # MSE Hessian with regularization\n",
    "\n",
    "        # Use pseudo-inverse to compute Newton step\n",
    "        return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** Added documentation and static typing, refactored `predict_proba()` to use list comprehension, and added the `get_params()` and `set_params()` methods to allow for compatibility with Scikit-Learn's cross-validation tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    \"\"\"MultiClass Logistic Regression using One-Versus-All approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.0001)\n",
    "        Constant applied to the regularization term.\n",
    "    solver : class, optional (default=HessianBinaryLogisticRegression)\n",
    "        Solver class to use for binary logistic regression.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eta: float,\n",
    "        iterations: int = 20,\n",
    "        C: float = 0.0001,\n",
    "        regularization: str = \"L2\",\n",
    "        solver: any = HessianBinaryLogisticRegression,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the MultiClassLogisticRegression object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : float\n",
    "            Learning rate.\n",
    "        iterations : int, optional (default=20)\n",
    "            Number of iterations for the solver.\n",
    "        C : float, optional (default=0.0001)\n",
    "            Constant applied to the regularization term.\n",
    "        regularization : str, optional (default=\"L2\")\n",
    "            Type of regularization to use.\n",
    "            Options are \"L1\" for Lasso, \"L2\" for Ridge, or \"Both\"\n",
    "            for Elastic Net.\n",
    "        solver : class, optional (default=HessianBinaryLogisticRegression)\n",
    "            Solver class to use for binary logistic regression.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.regularization = regularization\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return a message for the MultiClassLogisticRegression object.\"\"\"\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"MultiClass Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        return \"Untrained MultiClass Logistic Regression Object\"\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        \"\"\"Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "        y : np.array\n",
    "            Target labels.\n",
    "\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y))  # get each unique class value\n",
    "        self.classifiers_ = []\n",
    "        for i, yval in enumerate(self.unique_):  # for each unique value\n",
    "            y_binary = np.array(y == yval).astype(\n",
    "                int\n",
    "            )  # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "\n",
    "            hblr = self.solver(\n",
    "                eta=self.eta,\n",
    "                iterations=self.iters,\n",
    "                C=self.C,\n",
    "                regularization=self.regularization,\n",
    "            )\n",
    "            hblr.fit(X, y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "    def predict_proba(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the probability of each class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "\n",
    "        \"\"\"\n",
    "        probs = [\n",
    "            hblr.predict_proba(X).reshape((len(X), 1))\n",
    "            for hblr in self.classifiers_\n",
    "        ]\n",
    "        return np.hstack(probs)  # make into single matrix\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the discrete labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.unique_[\n",
    "            np.argmax(self.predict_proba(X), axis=1)\n",
    "        ]  # take argmax along row\n",
    "\n",
    "    def get_params(self, deep=True) -> dict:\n",
    "        \"\"\"Get the parameters of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        deep : bool, optional (default=True)\n",
    "            Required parameter for compatibility with sklearn.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Parameters of the model.\n",
    "\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"eta\": self.eta,\n",
    "            \"iterations\": self.iters,\n",
    "            \"C\": self.C,\n",
    "            \"solver\": self.solver,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params: dict) -> None:\n",
    "        \"\"\"Set the parameters of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **params : dict\n",
    "            Parameters to set.\n",
    "\n",
    "        \"\"\"\n",
    "        for key, value in params.items():\n",
    "            if key in self.get_params():\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid parameter: {key}\")\n",
    "        # re-initialize the solver with the new parameters\n",
    "        self.solver = getattr(__import__(__name__), self.solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Train/Test Split with 10-Fold CV on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultiClassLogisticRegression(\n",
    "    solver=HessianBinaryLogisticRegression, eta=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48224607762180016"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train.values, y_train.values)\n",
    "clf.predict(x_test.values)\n",
    "accuracy_score(y_test, clf.predict(x_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.4427\n",
      "Standard Deviation of Accuracy: 0.0514\n"
     ]
    }
   ],
   "source": [
    "# Perform 10-fold cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=clf,\n",
    "    X=customers_filtered.drop(columns=[\"Segmentation\"]),\n",
    "    y=customers_filtered[\"Segmentation\"],\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Calculate the mean accuracy and standard deviation\n",
    "mean_accuracy = np.mean(cv_scores)\n",
    "std_accuracy = np.std(cv_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.fharrell.com/post/split-val/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
