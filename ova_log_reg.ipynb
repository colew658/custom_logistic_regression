{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom One-Versus-All Logistic Regression\n",
    "\n",
    "By: Haiyan Cai, Joe Sellett, and Cole Wagner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import pinv\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv(\"customer_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results. For example, would the model be deployed or used mostly for offline analysis? As in previous labs, also detail how good the classifier needs to perform in order to be useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was collected by an automobile company for the purpose of customer segmentation. Customer segmentation is used by businesses to group customers into categories based on their purchasing behaviors. These behaviors may be influenced by factors such as income levels, previous spending habits, demographics, or any other data the company can obtain. This dataset was gathered because the automobile company aims to expand its current product offerings into new markets and seeks to understand potential customers in these markets. The company has successfully categorized its existing customers into four segments (A, B, C, D). The goal of our project is to accurately identify customer segments based on various variables, including gender, age, work experience, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer segmentation is a crucial practice for marketers across all industries. A successful categorization of customer segments will be highly valuable to automobile manufacturers worldwide, enabling them to determine how to allocate their marketing resources and tailor their strategies effectively. To be considered useful, our model must outperform existing models. This dataset was previously used in a competition where the winners achieved 95% accuracy in identifying customer segments. However, we believe a model with an AUC above 0.8 would generally be considered valuable in this business context, while a model achieving an AUC above 0.9 would certainly be considered highly effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] (mostly the same processes as from previous labs) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis (give reasoning). Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). Provide a breakdown of the variables after preprocessing (such as the mean, std, etc. for all variables, including numeric and categorical). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ever_Married</th>\n",
       "      <th>Age</th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Spending_Score</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Var_1</th>\n",
       "      <th>Segmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>462809</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>22</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Cat_4</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>462643</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Average</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Cat_4</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>466315</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>461735</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>462669</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Gender Ever_Married  Age Graduated     Profession  Work_Experience  \\\n",
       "0  462809    Male           No   22        No     Healthcare              1.0   \n",
       "1  462643  Female          Yes   38       Yes       Engineer              NaN   \n",
       "2  466315  Female          Yes   67       Yes       Engineer              1.0   \n",
       "3  461735    Male          Yes   67       Yes         Lawyer              0.0   \n",
       "4  462669  Female          Yes   40       Yes  Entertainment              NaN   \n",
       "\n",
       "  Spending_Score  Family_Size  Var_1 Segmentation  \n",
       "0            Low          4.0  Cat_4            D  \n",
       "1        Average          3.0  Cat_4            A  \n",
       "2            Low          1.0  Cat_6            B  \n",
       "3           High          2.0  Cat_6            B  \n",
       "4           High          6.0  Cat_6            A  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the ID and Var_1 variables from the data. The ID variable is unique to each row, so it does not provide any modeling value. Var_1 is anonymised, so we cannot know what it represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_filtered = customers.drop(columns=[\"ID\", \"Var_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "719"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(customers_filtered.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code, we can see that we have 796 duplicate rows, which we will remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_filtered.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender               0\n",
       "Ever_Married       140\n",
       "Age                  0\n",
       "Graduated           77\n",
       "Profession         123\n",
       "Work_Experience    786\n",
       "Spending_Score       0\n",
       "Family_Size        326\n",
       "Segmentation         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code, we can see that we must deal with missing values in five out of the nine remaining columns. We will deal with each column individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ever_Married"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ever_Married\n",
       "Yes    4325\n",
       "No     2884\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Ever_Married.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the number of missing values is relatively small for this column, we will use KNN imputation to fill in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Ever_Married to binary\n",
    "customers_filtered.Ever_Married = customers_filtered.Ever_Married.map(\n",
    "    {\"Yes\": 1, \"No\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_obj = KNNImputer(n_neighbors=5)\n",
    "customers_filtered[\"Ever_Married\"] = knn_obj.fit_transform(\n",
    "    customers_filtered[[\"Ever_Married\"]]\n",
    "    # round to nearest integer to get discrete value\n",
    ").round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ever_Married\n",
       "1.0    4465\n",
       "0.0    2884\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Ever_Married.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that KNNImputer imputed all missing values to 1 or \"Yes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graduated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graduated\n",
       "Yes    4579\n",
       "No     2693\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Graduated.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the number of missing values is relatively small for this column, we will use KNN imputation to fill in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Graduated to binary\n",
    "customers_filtered.Graduated = customers_filtered.Graduated.map(\n",
    "    {\"Yes\": 1, \"No\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_filtered[\"Graduated\"] = knn_obj.fit_transform(\n",
    "    customers_filtered[[\"Graduated\"]]\n",
    "    # round to nearest integer to get discrete value\n",
    ").round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graduated\n",
       "1.0    4656\n",
       "0.0    2693\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Graduated.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that all values of Graduated were imputed to 1 or \"Yes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profession\n",
       "Artist           2254\n",
       "Healthcare       1029\n",
       "Entertainment     920\n",
       "Engineer          677\n",
       "Doctor            676\n",
       "Lawyer            570\n",
       "Executive         570\n",
       "Marketing         290\n",
       "Homemaker         240\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Profession.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is a relatively small amount of missing values for this column, we will impute. However, because there are more than two categories in this variable, we will use mode imputation rather than KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/68j7w1pn443f9hrnsrlvphpw0000gn/T/ipykernel_59015/4012769783.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  customers_filtered.Profession.fillna(\"Artist\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values with \"Artist\" (mode)\n",
    "customers_filtered.Profession.fillna(\"Artist\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profession\n",
       "Artist           2377\n",
       "Healthcare       1029\n",
       "Entertainment     920\n",
       "Engineer          677\n",
       "Doctor            676\n",
       "Lawyer            570\n",
       "Executive         570\n",
       "Marketing         290\n",
       "Homemaker         240\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Profession.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Work_Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Work_Experience\n",
       "1.0     2039\n",
       "0.0     2032\n",
       "9.0      452\n",
       "8.0      451\n",
       "2.0      278\n",
       "3.0      248\n",
       "4.0      243\n",
       "6.0      202\n",
       "5.0      188\n",
       "7.0      188\n",
       "10.0      53\n",
       "11.0      50\n",
       "12.0      48\n",
       "13.0      46\n",
       "14.0      45\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Work_Experience.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a numeric column with a relatively small amount of missing values for this column, we will impute using KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with \"Artist\" (mode)\n",
    "customers_filtered[\"Work_Experience\"] = knn_obj.fit_transform(\n",
    "    customers_filtered[[\"Work_Experience\"]]\n",
    "    # round to nearest integer to get discrete value\n",
    ").round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Work_Experience\n",
       "1.0     2039\n",
       "0.0     2032\n",
       "3.0     1034\n",
       "9.0      452\n",
       "8.0      451\n",
       "2.0      278\n",
       "4.0      243\n",
       "6.0      202\n",
       "5.0      188\n",
       "7.0      188\n",
       "10.0      53\n",
       "11.0      50\n",
       "12.0      48\n",
       "13.0      46\n",
       "14.0      45\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Work_Experience.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all missing values were imputed to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Family_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Family_Size\n",
       "2.0    2199\n",
       "1.0    1350\n",
       "3.0    1341\n",
       "4.0    1184\n",
       "5.0     557\n",
       "6.0     203\n",
       "7.0      96\n",
       "8.0      49\n",
       "9.0      44\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Family_Size.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a numeric column with a relatively small amount of missing values for this column, we will impute using KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with \"Artist\" (mode)\n",
    "customers_filtered[\"Family_Size\"] = knn_obj.fit_transform(\n",
    "    customers_filtered[[\"Family_Size\"]]\n",
    "    # round to nearest integer to get discrete value\n",
    ").round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Family_Size\n",
       "2.0    2199\n",
       "3.0    1667\n",
       "1.0    1350\n",
       "4.0    1184\n",
       "5.0     557\n",
       "6.0     203\n",
       "7.0      96\n",
       "8.0      49\n",
       "9.0      44\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_filtered.Family_Size.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all missing values were imputed to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prepare the data for modeling, we need to normalize all numeric columns and one-hot encode all categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"Age\", \"Work_Experience\", \"Family_Size\"]\n",
    "cat_cols = customers_filtered.columns.difference(\n",
    "    num_cols + [\"Segmentation\"]\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical columns\n",
    "scaler = StandardScaler()\n",
    "customers_filtered[num_cols] = scaler.fit_transform(\n",
    "    customers_filtered[num_cols]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_cat_cols = one_hot_encoder.fit_transform(\n",
    "    customers_filtered[cat_cols]\n",
    ")\n",
    "\n",
    "# Create a dataframe with the encoded columns\n",
    "encoded_cat_df = pd.DataFrame(\n",
    "    encoded_cat_cols,\n",
    "    columns=one_hot_encoder.get_feature_names_out(cat_cols),\n",
    "    index=customers_filtered.index,  # Ensure the index matches the original dataframe\n",
    ")\n",
    "\n",
    "# Concatenate the encoded columns with the original dataframe\n",
    "customers_filtered = pd.concat(\n",
    "    [customers_filtered.drop(columns=cat_cols), encoded_cat_df], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Divide your data into training and testing splits using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustomers_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSegmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustomers_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSegmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustomers_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSegmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2872\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2868\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2870\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2872\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2874\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m   2876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2877\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2878\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2879\u001b[0m     )\n\u001b[1;32m   2880\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2405\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2401\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe groups parameter is ignored by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2403\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   2404\u001b[0m     )\n\u001b[0;32m-> 2405\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/sklearn/utils/validation.py:105\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_api \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(X\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex floating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    customers_filtered.drop(columns=[\"Segmentation\"]),\n",
    "    customers_filtered[\"Segmentation\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=customers_filtered[\"Segmentation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Class (Steepest Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** Documentation and type hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    \"\"\"Binary Logistic Regression using gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Constant applied to the regularization term.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eta: float,\n",
    "        iterations: int = 20,\n",
    "        C: float = 0.001,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the BinaryLogisticRegression object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : float\n",
    "            Learning rate.\n",
    "        iterations : int, optional (default=20)\n",
    "            Number of iterations for the solver.\n",
    "        C : float, optional (default=0.001)\n",
    "            Constant applied to the regularization term.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return a message for the BinaryLogisticRegression object.\"\"\"\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"Binary Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        return \"Untrained Binary Logistic Regression Object\"\n",
    "\n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X: np.array) -> np.array:\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))  # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta: np.array) -> np.array:\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta)  # 1/(1+np.exp(-theta))\n",
    "\n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self, X: np.array, y: np.array) -> np.array:\n",
    "        ydiff = (\n",
    "            y - self.predict_proba(X, add_bias=False).ravel()\n",
    "        )  # get y difference\n",
    "        gradient = np.mean(\n",
    "            X * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    # public:\n",
    "    def predict_proba(\n",
    "        self, X: np.array, add_bias: bool = True\n",
    "    ) -> np.array:\n",
    "        \"\"\"Predict the probability of the positive class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "        add_bias : bool, optional (default=True)\n",
    "            Whether to add a bias term to the input data.\n",
    "\n",
    "        \"\"\"\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)  # return the probability y=1\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the discrete labels based on a cutoff of p > 0.5.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.predict_proba(X) > 0.5  # return the actual prediction\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        \"\"\"Fit the model to data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "        y : np.array\n",
    "            Target labels.\n",
    "\n",
    "        \"\"\"\n",
    "        Xb = self._add_bias(X)  # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros(\n",
    "            (num_features, 1)\n",
    "        )  # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb, y)\n",
    "            self.w_ += gradient * self.eta  # multiply by learning rate\n",
    "            # add bacause maximizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** documentation and static typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    \"\"\"Logistic Regression using stochastic gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Constant applied to the regularization term.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # stochastic gradient calculation\n",
    "    def _get_gradient(self, X: np.array, y: np.array) -> np.array:\n",
    "        # grab a subset of samples in a mini-batch\n",
    "        mini_batch_size = 50\n",
    "        idxs = np.random.choice(len(y), mini_batch_size)\n",
    "\n",
    "        ydiff = (\n",
    "            y[idxs] - self.predict_proba(X[idxs], add_bias=False).ravel()\n",
    "        )  # get y difference (now scalar)\n",
    "        gradient = np.mean(\n",
    "            X[idxs] * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** documentation and static typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \"\"\"Logistic Regression using Newton's method for optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.001)\n",
    "        Constant applied to the regularization term.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self, X: np.array, y: np.array) -> np.array:\n",
    "        g = self.predict_proba(\n",
    "            X, add_bias=False\n",
    "        ).ravel()  # get sigmoid value for all classes\n",
    "        hessian = (\n",
    "            X.T @ np.diag(g * (1 - g)) @ X - 2 * self.C\n",
    "        )  # calculate the hessian\n",
    "\n",
    "        ydiff = y - g  # get y difference\n",
    "        gradient = np.sum(\n",
    "            X * ydiff[:, np.newaxis], axis=0\n",
    "        )  # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        # Note the pinv() to make the hessian function inverse\n",
    "        return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definition pulled from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications:** Added documentation, refactored `predict_proba()` to use list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    \"\"\"MultiClass Logistic Regression using One-Versus-All approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    iterations : int, optional (default=20)\n",
    "        Number of iterations for the solver.\n",
    "    C : float, optional (default=0.0001)\n",
    "        Constant applied to the regularization term.\n",
    "    solver : class, optional (default=HessianBinaryLogisticRegression)\n",
    "        Solver class to use for binary logistic regression.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eta: float,\n",
    "        iterations: int = 20,\n",
    "        C: float = 0.0001,\n",
    "        solver: any = HessianBinaryLogisticRegression,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the MultiClassLogisticRegression object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : float\n",
    "            Learning rate.\n",
    "        iterations : int, optional (default=20)\n",
    "            Number of iterations for the solver.\n",
    "        C : float, optional (default=0.0001)\n",
    "            Constant applied to the regularization term.\n",
    "        solver : class, optional (default=HessianBinaryLogisticRegression)\n",
    "            Solver class to use for binary logistic regression.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return a message for the MultiClassLogisticRegression object.\"\"\"\n",
    "        if hasattr(self, \"w_\"):\n",
    "            return (\n",
    "                \"MultiClass Logistic Regression Object with coefficients:\\n\"\n",
    "                + str(self.w_)\n",
    "            )  # is we have trained the object\n",
    "        return \"Untrained MultiClass Logistic Regression Object\"\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        \"\"\"Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "        y : np.array\n",
    "            Target labels.\n",
    "\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y))  # get each unique class value\n",
    "        self.classifiers_ = []\n",
    "        for i, yval in enumerate(self.unique_):  # for each unique value\n",
    "            y_binary = np.array(y == yval).astype(\n",
    "                int\n",
    "            )  # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "\n",
    "            hblr = self.solver(\n",
    "                eta=self.eta, iterations=self.iters, C=self.C\n",
    "            )\n",
    "            hblr.fit(X, y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "    def predict_proba(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the probability of each class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "\n",
    "        \"\"\"\n",
    "        probs = [\n",
    "            hblr.predict_proba(X).reshape((len(X), 1))\n",
    "            for hblr in self.classifiers_\n",
    "        ]\n",
    "        return np.hstack(probs)  # make into single matrix\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predict the discrete labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.unique_[\n",
    "            np.argmax(self.predict_proba(X), axis=1)\n",
    "        ]  # take argmax along row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
